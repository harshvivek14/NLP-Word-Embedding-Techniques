{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip gram and CBOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MyJGHC1BaWl"
   },
   "source": [
    "We will built the Skipgram and CBOW models from scratch, train them on a relatively small corpus, i.e, on BBC Data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vkdi2YQiz3Fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMNFODiS2UXU",
    "outputId": "ae02bceb-305a-45d8-9428-adecb4d9a670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           category                                               text\n",
      "0              tech  tv future in the hands of viewers with home th...\n",
      "1          business  worldcom boss  left books alone  former worldc...\n",
      "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
      "3             sport  yeading face newcastle in fa cup premiership s...\n",
      "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
      "...             ...                                                ...\n",
      "2220       business  cars pull down us retail figures us retail sal...\n",
      "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
      "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
      "2223       politics  how political squabbles snowball it s become c...\n",
      "2224          sport  souness delight at euro progress boss graeme s...\n",
      "\n",
      "[2225 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bbc-text.csv')\n",
    "print(df)\n",
    "sentences = ''\n",
    "articles = list(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krCIiau6BNVB"
   },
   "source": [
    "## Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUsTuWeG9_Km",
    "outputId": "82ec76f4-dc73-40b5-ed61-c84eed6b6cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 1.22 ms, total: 50.2 ms\n",
      "Wall time: 49.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in articles[:200]:\n",
    "    sentences += i.split('.')\n",
    "\n",
    "# Remove sentences with fewer than 3 words\n",
    "corpus = [sentence for sentence in sentences if sentence.count(\" \") >= 5]\n",
    "\n",
    "# Remove punctuation in text and fit tokenizer on entire corpus\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert text to sequence of integer values\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "n_samples = sum(len(s) for s in corpus) # Total number of words in the corpus\n",
    "V = len(tokenizer.word_index) + 1 # Total number of unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkpMdk0W_Tvb",
    "outputId": "3d7b2189-5295-4738-cf5f-a331cc04a8ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74390, 9322)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8H_KWQi_Vg-",
    "outputId": "d7a0d387-cdbf-413e-99c3-94c65cf1cd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('to', 2), ('of', 3), ('and', 4), ('a', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Example of how word to integer mapping looks like in the tokenizer\n",
    "print(list((tokenizer.word_index.items()))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "49Vrpesb_X5v"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "window_size = 2\n",
    "window_size_corpus = 4\n",
    "\n",
    "# Set numpy seed for reproducible results\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9Yh1m9sn_auh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data for the skipgram model\n",
    "# The function returns two arrays: all_in, which contains the target words, and \n",
    "# all_out, which contains the corresponding one-hot encoded context words.\n",
    "\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    maxlen = window_size * 2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "\n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    # Add the input word\n",
    "                    all_in.append(word)\n",
    "                    # Add one-hot of the context words\n",
    "                    all_out.append(to_categorical(words[i], V))\n",
    "\n",
    "    return (np.array(all_in), np.array(all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0dDFx_a_b9q",
    "outputId": "239052ee-cd76-44a3-90c4-d708f4bdea9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 3.28 s, total: 4.95 s\n",
      "Wall time: 6.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((275978,), (275978, 9322))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create training data\n",
    "X_skip, y_skip = generate_data_skipgram(corpus, window_size, V)\n",
    "X_skip.shape, y_skip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208 208 407 407 407   6   6   6   6   1   1   1   1]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_skip[0:13])\n",
    "print(y_skip[0:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFII7Bz5_dXp",
    "outputId": "1e22e91c-58ba-4ac2-e83b-a798e6507e80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 300)            2796600   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9322)              2805922   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,602,522\n",
      "Trainable params: 5,602,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "CPU times: user 53.5 ms, sys: 32.2 ms, total: 85.7 ms\n",
      "Wall time: 54.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create skipgram architecture\n",
    "\n",
    "dim = 300\n",
    "skipgram_models = []\n",
    "\n",
    "# Initialize a Keras Sequential model\n",
    "skipgram = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "skipgram.add(Embedding(input_dim=V,\n",
    "                        output_dim=dim,\n",
    "                        input_length=1,\n",
    "                        embeddings_initializer='glorot_uniform'))\n",
    "\n",
    "# Add a Reshape layer, which reshapes the output of the embedding layer (1,dim) to (dim,)\n",
    "skipgram.add(Reshape((dim, )))\n",
    "\n",
    "# Add a final Dense layer with the same size as in [1]\n",
    "skipgram.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Compile the model with a suitable loss function and select an optimizer.\n",
    "# Optimizer Adagrad was used in paper\n",
    "skipgram.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "skipgram.summary()\n",
    "print(\"\")\n",
    "skipgram_models.append(skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWouhLLB_gpb",
    "outputId": "f89aed64-39fe-4b7e-cc3c-c60bc5cd77b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 05:27:27.713762: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4313/4313 [==============================] - 59s 14ms/step - loss: 7.3100 - accuracy: 0.0671\n",
      "Epoch 2/10\n",
      "4313/4313 [==============================] - 62s 14ms/step - loss: 6.7750 - accuracy: 0.0789\n",
      "Epoch 3/10\n",
      "4313/4313 [==============================] - 64s 15ms/step - loss: 6.3873 - accuracy: 0.0861\n",
      "Epoch 4/10\n",
      "4313/4313 [==============================] - 61s 14ms/step - loss: 6.0007 - accuracy: 0.0892\n",
      "Epoch 5/10\n",
      "4313/4313 [==============================] - 59s 14ms/step - loss: 5.6642 - accuracy: 0.0873\n",
      "Epoch 6/10\n",
      "4313/4313 [==============================] - 58s 13ms/step - loss: 5.4052 - accuracy: 0.0839\n",
      "Epoch 7/10\n",
      "4313/4313 [==============================] - 57s 13ms/step - loss: 5.2267 - accuracy: 0.0812\n",
      "Epoch 8/10\n",
      "4313/4313 [==============================] - 60s 14ms/step - loss: 5.1146 - accuracy: 0.0790\n",
      "Epoch 9/10\n",
      "4313/4313 [==============================] - 58s 13ms/step - loss: 5.0479 - accuracy: 0.0777\n",
      "Epoch 10/10\n",
      "4313/4313 [==============================] - 59s 14ms/step - loss: 5.0095 - accuracy: 0.0763\n",
      "\n",
      "CPU times: user 39min 43s, sys: 10min 50s, total: 50min 34s\n",
      "Wall time: 9min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Training the skipgram models\n",
    "for skipgram in skipgram_models:\n",
    "    skipgram.fit(X_skip, y_skip, batch_size=64, epochs=10, verbose=1)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cldfl4HP_jAE"
   },
   "outputs": [],
   "source": [
    "\n",
    "for skipgram in skipgram_models:\n",
    "    # Save embeddings for vectors of length 50, 150 and 300 using skipgram model\n",
    "    weights = skipgram.get_weights()\n",
    "\n",
    "    # Get the embedding matrix\n",
    "    embedding = weights[0]\n",
    "\n",
    "    # Get word embeddings for each word in the vocabulary, write to file\n",
    "    f = open(f\"vectors_skipgram_{len(embedding[0])}.txt\", \"w\")\n",
    "\n",
    "    # Create columns for the words and the values in the matrix, makes it easier to read as dataframe\n",
    "    columns = [\"word\"] + [f\"value_{i+1}\" for i in range(embedding.shape[1])]\n",
    "\n",
    "    # Start writing to the file, start with the column names\n",
    "    f.write(\" \".join(columns))\n",
    "\n",
    "    # Start a new line\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5nkl5mF_n1H",
    "outputId": "912e9445-ddc8-47b5-da78-57732bc7684c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00779338,  0.01972531,  0.01419954, ..., -0.02101514,\n",
       "         0.01292091,  0.02110568],\n",
       "       [-0.09163388, -0.02635568,  0.07773658, ...,  0.01926036,\n",
       "        -0.03169642,  0.1490469 ],\n",
       "       [ 0.16329095, -0.20949455, -0.10299218, ...,  0.18321548,\n",
       "        -0.05173499,  0.0817973 ],\n",
       "       ...,\n",
       "       [ 0.37874895, -0.01725492,  0.3530954 , ..., -0.06208007,\n",
       "        -0.08999639,  0.09179325],\n",
       "       [ 0.01896744, -0.25777036, -0.20103447, ..., -0.03838814,\n",
       "         0.0492724 ,  0.22370675],\n",
       "       [-0.08725878,  0.01326364,  0.20278992, ..., -0.5401249 ,\n",
       "         0.01658141,  0.24005595]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MMXBpKICNld",
    "outputId": "b68d04cf-3b5f-4fb0-ac5e-7dddcd1016e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipgram.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QM00BJo8CZgf",
    "outputId": "6789022e-a4b0-4184-89d8-3ffd11177b33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9322"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipgram.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mx6NU83WCaL1",
    "outputId": "f9e59a23-3f98-4465-b92c-6cf1253ee5d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipgram.get_weights()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwHlFJ4VCc7J",
    "outputId": "2b36fc66-f88f-4e44-883d-847a7e4234b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09163388, -0.02635568,  0.07773658,  0.08947439,  0.15617695,\n",
       "       -0.15438677, -0.11931635,  0.14262363, -0.08654329,  0.14073399,\n",
       "       -0.11981915,  0.07665411, -0.00315618,  0.02960972,  0.04841485,\n",
       "        0.12730306, -0.02558832, -0.06607738, -0.16928354,  0.04165031,\n",
       "        0.21741606, -0.0013554 , -0.0768519 , -0.04429504, -0.08227623,\n",
       "        0.09248411, -0.13111536, -0.09208474,  0.35656106, -0.06911632,\n",
       "        0.02089642,  0.04338039, -0.13249621, -0.03986635, -0.17646879,\n",
       "       -0.07252575, -0.13664111, -0.07110459, -0.07233532,  0.04831076,\n",
       "       -0.03079399, -0.04340337,  0.11197045,  0.0440909 ,  0.11237387,\n",
       "       -0.06821577, -0.0976437 ,  0.06544045, -0.0836357 ,  0.12521996,\n",
       "       -0.21540208, -0.17066208, -0.07709809,  0.06175427, -0.13189538,\n",
       "        0.11937968,  0.07496584,  0.03332197, -0.08093476, -0.14861879,\n",
       "        0.10410377,  0.11623706, -0.0116831 , -0.12481596,  0.06477566,\n",
       "        0.13426732,  0.1289558 ,  0.0925407 ,  0.04530833,  0.12694515,\n",
       "        0.2571379 ,  0.11025546, -0.04868571, -0.10726155,  0.13616252,\n",
       "       -0.05796213, -0.0310859 ,  0.04452335,  0.08611754, -0.04440124,\n",
       "       -0.04710489, -0.04636125,  0.05768852,  0.07575168,  0.07930849,\n",
       "        0.00443881,  0.1572006 , -0.07547684, -0.05148585,  0.12652089,\n",
       "        0.14578606, -0.0535087 ,  0.0624217 , -0.07832859,  0.33044466,\n",
       "       -0.17896855, -0.00837188, -0.00850574,  0.07679668, -0.03923381,\n",
       "       -0.2129357 , -0.00225626, -0.0543834 , -0.00611058,  0.00376514,\n",
       "       -0.07716426, -0.06413893, -0.11122715,  0.05711274, -0.06235793,\n",
       "       -0.20170683,  0.0609125 ,  0.03629361, -0.14660105,  0.21156892,\n",
       "       -0.0277571 , -0.03738785, -0.14841338, -0.5945646 , -0.21449754,\n",
       "        0.0684291 , -0.11914956,  0.13569754,  0.16854134,  0.04590807,\n",
       "        0.05871545,  0.16847098,  0.1555105 ,  0.08515565,  0.05169959,\n",
       "       -0.05440642,  0.01472731, -0.09669966,  0.26109475,  0.10622704,\n",
       "       -0.0231045 , -0.23233174,  0.1394369 ,  0.04513191,  0.1897108 ,\n",
       "        0.10344261, -0.00559726,  0.07153683,  0.29189712,  0.12618369,\n",
       "        0.08512162,  0.03610773, -0.12308504,  0.11581823,  0.115217  ,\n",
       "        0.00859413,  0.20992225, -0.00766519, -0.12534551, -0.12251135,\n",
       "        0.14310147, -0.18861033, -0.00596559,  0.14431958, -0.0977821 ,\n",
       "        0.20785786,  0.11410891,  0.07096664,  0.12702283,  0.11191443,\n",
       "        0.11150707, -0.06575944, -0.12217135, -0.11055651, -0.17489697,\n",
       "       -0.12471473, -0.17173529, -0.06002153,  0.14716104, -0.06972157,\n",
       "        0.08767456, -0.12003955, -0.12876426,  0.08032615, -0.04145365,\n",
       "        0.01847123, -0.0157175 , -0.07261546,  0.07691515,  0.08145157,\n",
       "       -0.12486248, -0.14910997,  0.12995552,  0.16509992, -0.0568774 ,\n",
       "        0.02550176,  0.00427084,  0.03727131, -0.08804162, -0.2963719 ,\n",
       "        0.16416416,  0.0381522 ,  0.15633033, -0.1314143 ,  0.13056539,\n",
       "        0.05724723,  0.08948862,  0.07260787,  0.24050875, -0.09477373,\n",
       "        0.04165826,  0.06841955, -0.05062314, -0.1268075 ,  0.03699207,\n",
       "       -0.15984234,  0.07631955, -0.07815719,  0.10854593,  0.0053746 ,\n",
       "       -0.063908  , -0.039688  , -0.08304745, -0.1564428 , -0.02741539,\n",
       "       -0.10473455,  0.05172119,  0.15401201,  0.13311896,  0.14542684,\n",
       "       -0.01177567, -0.12919067, -0.05444377,  0.09054058, -0.00327842,\n",
       "       -0.09085523,  0.0608656 ,  0.20879129,  0.08937819, -0.13080989,\n",
       "        0.04620695,  0.06011073,  0.21558079,  0.30848715,  0.05216813,\n",
       "       -0.00227793,  0.01384507,  0.03081664,  0.1874463 , -0.13646962,\n",
       "        0.1976281 , -0.02859307, -0.05338164, -0.09269187,  0.22062904,\n",
       "       -0.18189237, -0.01063139,  0.13773611,  0.07879499,  0.04404861,\n",
       "        0.0884297 , -0.11431383, -0.06938428,  0.10368774, -0.06126789,\n",
       "       -0.00662969,  0.07493668, -0.13125597,  0.13864827,  0.19578706,\n",
       "       -0.10191497, -0.13360119, -0.09791487,  0.07193676, -0.08348008,\n",
       "        0.11924175, -0.22615297,  0.03373815,  0.07466235,  0.0950165 ,\n",
       "       -0.07443098, -0.06667228, -0.09528076, -0.10918709,  0.00828919,\n",
       "       -0.23580755, -0.05109461, -0.08718406,  0.06982371,  0.32167804,\n",
       "        0.06915833, -0.01921475,  0.21382439,  0.02508617, -0.1678531 ,\n",
       "        0.09203675, -0.12329727, -0.07602825, -0.06812961,  0.02495784,\n",
       "       -0.1346004 ,  0.04629885,  0.01926036, -0.03169642,  0.1490469 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.get_weights()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WndM_dgtBsB3"
   },
   "source": [
    "To get the word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "stGNMknWCga3"
   },
   "outputs": [],
   "source": [
    "index = tokenizer.word_index['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQIeSU9YCqLk",
    "outputId": "717bbd68-0248-4615-ef62-a207427cbd8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27446085e-01,  9.77579132e-02, -2.30674505e-01, -5.93292229e-02,\n",
       "       -1.31638408e-01, -5.27725637e-01, -6.47630811e-01, -6.72960877e-02,\n",
       "       -2.58396268e-02,  1.60156917e-02,  7.50044659e-02, -2.78036743e-01,\n",
       "       -1.02429293e-01, -3.28077190e-02,  4.93224487e-02, -2.48573691e-01,\n",
       "       -2.88832426e-01, -5.05673289e-01, -1.46271408e-01,  6.91479594e-02,\n",
       "       -2.97959328e-01, -8.52769464e-02, -6.10925630e-02, -2.45853797e-01,\n",
       "        1.58908412e-01,  4.77355540e-01,  1.39180496e-01,  7.22390227e-03,\n",
       "        9.57334414e-02, -4.00800584e-03, -4.19922590e-01,  2.61898398e-01,\n",
       "        2.37221509e-01,  1.05325691e-03,  2.73265511e-01,  2.07189262e-01,\n",
       "        5.26713729e-01, -1.86639935e-01, -2.68109739e-01, -1.41160205e-01,\n",
       "       -7.61407688e-02, -1.87115476e-01,  1.74072847e-01,  1.15389414e-01,\n",
       "       -1.74125761e-01, -3.69330138e-01,  1.39214039e-01, -7.86893591e-02,\n",
       "       -4.91450191e-01,  1.48090109e-01,  2.78602894e-02,  6.70169294e-02,\n",
       "        2.70318270e-01, -1.95403650e-01, -2.32429102e-01, -5.01666129e-01,\n",
       "        6.04722559e-01,  2.65886158e-01,  3.97630408e-02, -6.57037199e-02,\n",
       "        8.90054777e-02,  2.17028216e-01,  1.90319836e-01, -2.15134710e-01,\n",
       "        8.66173506e-02,  3.65367115e-01, -9.56444666e-02,  1.90162867e-01,\n",
       "        8.99589956e-02,  3.92607570e-01, -2.58676231e-01, -1.17084533e-01,\n",
       "       -6.86485350e-01, -6.21722371e-04,  2.14775935e-01,  5.46964705e-01,\n",
       "       -3.36439878e-01,  1.04210563e-01, -8.18040147e-02, -2.33134270e-01,\n",
       "        2.18718678e-01,  3.53470296e-01,  1.91688508e-01, -1.00586765e-01,\n",
       "        2.00119819e-02,  2.31530145e-01, -5.02915144e-01, -5.73885202e-01,\n",
       "        2.16297045e-01, -1.31601393e-01, -9.55454707e-02, -3.78501266e-02,\n",
       "        5.30869484e-01, -3.57567817e-01, -7.62766004e-01,  1.54492155e-01,\n",
       "       -1.45724118e-01,  4.20813322e-01,  3.81469369e-01, -1.42444268e-01,\n",
       "       -1.74900234e-01,  2.55619138e-01,  6.09650053e-02, -4.11340058e-01,\n",
       "        1.24131672e-01, -3.87418568e-02, -1.06567897e-01, -8.58979076e-02,\n",
       "        1.67273641e-01, -1.19987004e-01,  2.63686895e-01, -5.75356662e-01,\n",
       "       -1.47039788e-02,  2.18181208e-01, -4.68437374e-02,  2.96708643e-01,\n",
       "        8.58470500e-02,  3.13318484e-02, -3.49224538e-01, -2.45930701e-01,\n",
       "        1.47388592e-01,  8.81400704e-02, -1.79585263e-01, -2.29759365e-01,\n",
       "        1.25354618e-01,  2.12159202e-01, -1.48222027e-02, -3.65422010e-01,\n",
       "        1.98127255e-01,  9.61885750e-02,  3.91672403e-01, -2.46611014e-01,\n",
       "        3.03696781e-01, -1.42516568e-01, -7.75763765e-02, -1.76528558e-01,\n",
       "       -1.27065003e-01,  2.15893406e-02,  1.99760899e-01,  4.75038476e-02,\n",
       "       -1.49866968e-01,  1.59616739e-01,  5.40817939e-02,  5.26360534e-02,\n",
       "        2.14661419e-01, -1.22989379e-01, -8.42349753e-02,  5.29485524e-01,\n",
       "        1.27898455e-01,  2.73198247e-01, -7.90878594e-01,  1.06288129e-02,\n",
       "        3.96745622e-01,  8.01351666e-02, -3.64811271e-01,  4.58076805e-01,\n",
       "       -1.76003858e-01, -6.42920256e-01,  6.44086823e-02,  2.37542838e-01,\n",
       "        1.82055205e-01, -2.30219543e-01,  3.70928377e-01, -1.00714915e-01,\n",
       "       -7.56773166e-03,  4.73191381e-01, -3.71153116e-01,  2.99728692e-01,\n",
       "       -1.05314963e-01, -2.62643576e-01,  5.69856875e-02,  8.16238075e-02,\n",
       "        2.20135391e-01, -1.25799835e-01, -1.96814984e-01,  2.40696386e-01,\n",
       "        3.37782145e-01, -1.01970613e-01,  5.23074046e-02,  7.35702217e-02,\n",
       "       -1.50810197e-01, -3.27371508e-01, -3.51484492e-02,  1.18181355e-01,\n",
       "       -3.56392413e-01, -1.88033476e-01,  1.63426742e-01, -4.17653352e-01,\n",
       "       -1.41301394e-01,  1.11788251e-01, -2.23925084e-01,  5.87314606e-01,\n",
       "       -1.05098180e-01, -3.73057276e-01, -2.10344672e-01,  1.67397521e-02,\n",
       "        1.11994103e-01, -4.11192141e-02,  3.19184601e-01, -4.59947437e-02,\n",
       "        4.61325467e-01, -1.25416443e-01, -4.08759341e-02, -7.46700913e-02,\n",
       "       -4.54760522e-01, -2.47564912e-01, -4.10663754e-01,  2.23769665e-01,\n",
       "        6.81522787e-02, -3.62974256e-01, -5.44494927e-01,  1.65870085e-01,\n",
       "        1.13061592e-01,  1.91671085e-02, -2.37846926e-01,  2.02811077e-01,\n",
       "       -3.31920683e-01, -4.59752381e-02, -4.78007883e-01,  1.19107664e-01,\n",
       "       -3.77733976e-01, -1.59210548e-01, -1.57044560e-01,  4.55076069e-01,\n",
       "       -1.54057473e-01, -2.00480223e-01, -5.82690304e-03, -2.34245449e-01,\n",
       "        6.60322845e-01, -7.59818107e-02, -6.26253635e-02, -1.17526300e-01,\n",
       "        9.25580040e-02, -1.10731356e-01,  6.52576908e-02, -1.32229716e-01,\n",
       "        1.38294905e-01, -8.76245350e-02, -3.11410934e-01, -2.74159014e-01,\n",
       "        3.70096639e-02, -4.92653251e-01,  4.14681107e-01, -3.00877869e-01,\n",
       "       -1.20138876e-01, -4.65447664e-01, -5.17915487e-02, -2.95132965e-01,\n",
       "        5.30606173e-02,  1.68658063e-01,  1.11061394e-01,  2.90688246e-01,\n",
       "       -2.72691548e-01,  2.53481150e-01, -2.32614040e-01, -4.43120301e-01,\n",
       "       -2.72400290e-01, -2.63348371e-01, -2.60446399e-01,  2.63838261e-01,\n",
       "       -1.01955451e-01, -1.24204092e-01, -3.46995324e-01,  1.70276966e-02,\n",
       "       -2.14890778e-01,  4.86368030e-01, -5.01823962e-01,  2.42365658e-01,\n",
       "        1.31584674e-01,  1.33455455e-01, -6.96450830e-01, -9.55160141e-01,\n",
       "       -3.62711966e-01, -4.87595238e-02, -1.27463788e-01,  5.44152185e-02,\n",
       "       -3.78779173e-01,  1.77075878e-01,  8.75365958e-02,  1.33584753e-01,\n",
       "        1.06533684e-01,  1.99086130e-01,  2.16078624e-01,  7.67489374e-02,\n",
       "        3.07849675e-01,  1.69970334e-01, -3.72916788e-01,  2.51395106e-01,\n",
       "        8.64068493e-02,  2.69533575e-01, -5.39331958e-02, -2.58967698e-01,\n",
       "        2.01247320e-01, -4.39794630e-01, -1.03799915e-02, -9.35335271e-03,\n",
       "       -3.01723868e-01, -2.31313780e-01,  2.93429196e-01, -7.69714564e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.get_weights()[0][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52erh42yFVzA",
    "outputId": "37de6c4c-dc5e-4063-904f-9464c5431e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 nearest words to 'king' are: \n",
      "('tote', 0.34790592104473206)\n",
      "('adaptation', 0.34316849037173064)\n",
      "('name', 0.32730016328623873)\n",
      "('macfarlane', 0.3272542045187647)\n",
      "('governor', 0.3254736139367036)\n",
      "('mervyn', 0.3173878787141464)\n",
      "('charles', 0.31286011151883036)\n",
      "('dead', 0.29567151691208293)\n",
      "('comeback', 0.2950116375327063)\n",
      "('duke', 0.29199742666194783)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your pre-trained word embeddings into a dictionary or a matrix\n",
    "# word_vectors should be a dictionary where keys are words and values are their corresponding vectors\n",
    "# Or word_vectors can be a matrix where rows correspond to words and columns are vector dimensions\n",
    "# You should replace this with your actual word embeddings\n",
    "\n",
    "# Sample code for loading pre-trained word vectors into a dictionary\n",
    "word_vectors = {}\n",
    "i=0\n",
    "\n",
    "target_word = \"king\"\n",
    "\n",
    "\n",
    "with open(\"vectors_skipgram_300.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = np.array([float(x) for x in parts[1:]])\n",
    "        word_vectors[word] = vector\n",
    "\n",
    "# Target word for which you want to find the k-nearest words\n",
    "\n",
    "\n",
    "# Calculate cosine similarities with all words in the vocabulary\n",
    "similarities = {}\n",
    "target_vector = word_vectors[target_word]\n",
    "for word, vector in word_vectors.items():\n",
    "    if word != target_word:\n",
    "        cosine_sim = cosine_similarity([target_vector], [vector])\n",
    "        similarities[word] = cosine_sim[0][0]\n",
    "\n",
    "# Sort the words by their cosine similarity scores in descending order\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top-k words as the k-nearest words\n",
    "k = 10  # Number of nearest words you want to find\n",
    "nearest_words = [(word, e) for word, e in sorted_similarities[:k]]\n",
    "\n",
    "# Print the k-nearest words\n",
    "print(f\"The {k} nearest words to '{target_word}' are: \")\n",
    "for i in (nearest_words):\n",
    "    print(i)\n",
    "\n",
    "\n",
    "skipgram_word_emd = word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVkTnQHpBC9G"
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mjFSAz4xM0Zs"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# The function returns two arrays: all_in, which contains the context words, \n",
    "# and all_out, which contains the corresponding one-hot encoded target words.\n",
    "\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "\n",
    "    # Iterate over all sentences\n",
    "    for sentence in corpus:\n",
    "        L = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            # Empty list which will store the context words\n",
    "            context_words = []\n",
    "            for i in range(start, end):\n",
    "                # Skip the 'same' word\n",
    "                if i != index:\n",
    "                    # Add a word as a context word if it is within the window size\n",
    "                    if 0 <= i < L:\n",
    "                        context_words.append(sentence[i])\n",
    "                    else:\n",
    "                        # Pad with zero if there are no words\n",
    "                        context_words.append(0)\n",
    "            # Append the list with context words\n",
    "            all_in.append(context_words)\n",
    "\n",
    "            # Add one-hot encoding of the target word\n",
    "            all_out.append(to_categorical(word, V))\n",
    "\n",
    "    return (np.array(all_in), np.array(all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an4kfPrHM4BV",
    "outputId": "c41af6ea-3637-48cc-b90e-e09bbcf8fd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 538 ms, sys: 986 ms, total: 1.52 s\n",
      "Wall time: 1.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((74390, 4), (74390, 9322))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Create the training data\n",
    "X_cbow, y_cbow = generate_data_cbow(corpus, window_size, V)\n",
    "X_cbow.shape, y_cbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0  407    6]\n",
      " [   0  208    6    1]\n",
      " [ 208  407    1 1865]\n",
      " [ 407    6 1865    3]\n",
      " [   6    1    3  785]\n",
      " [   1 1865  785   17]\n",
      " [1865    3   17  160]\n",
      " [   3  785  160  843]\n",
      " [ 785   17  843 1605]\n",
      " [  17  160 1605 5047]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_cbow[:10])\n",
    "print(y_cbow[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJBnZ-lAM7o0",
    "outputId": "1eb5dcbe-790d-4b87-8b4c-a1b12e7a3e63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 300)            2796600   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9322)              2805922   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,602,522\n",
      "Trainable params: 5,602,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "CPU times: user 43.2 ms, sys: 25.1 ms, total: 68.3 ms\n",
      "Wall time: 40.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the CBOW architecture\n",
    "cbow_models = []\n",
    "dim = 300\n",
    "cbow = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "cbow.add(Embedding(input_dim=V,\n",
    "                    output_dim=dim,\n",
    "                    input_length=window_size*2, # Note that we now have 2L words for each input entry\n",
    "                    embeddings_initializer='glorot_uniform'))\n",
    "\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim, )))\n",
    "\n",
    "cbow.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "cbow.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "cbow.summary()\n",
    "print(\"\")\n",
    "cbow_models.append(cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZywPe5gyM-O6",
    "outputId": "7170089a-0460-4b6e-d34a-a3a6db2d9bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1163/1163 [==============================] - 16s 13ms/step - loss: 7.4937 - accuracy: 0.0739\n",
      "Epoch 2/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 6.7223 - accuracy: 0.0940\n",
      "Epoch 3/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 6.3521 - accuracy: 0.1189\n",
      "Epoch 4/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 5.9609 - accuracy: 0.1477\n",
      "Epoch 5/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 5.5499 - accuracy: 0.1787\n",
      "Epoch 6/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 5.1358 - accuracy: 0.2100\n",
      "Epoch 7/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 4.7327 - accuracy: 0.2438\n",
      "Epoch 8/50\n",
      "1163/1163 [==============================] - 17s 15ms/step - loss: 4.3425 - accuracy: 0.2767\n",
      "Epoch 9/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 3.9681 - accuracy: 0.3127\n",
      "Epoch 10/50\n",
      "1163/1163 [==============================] - 17s 15ms/step - loss: 3.6091 - accuracy: 0.3519\n",
      "Epoch 11/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 3.2660 - accuracy: 0.3926\n",
      "Epoch 12/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 2.9400 - accuracy: 0.4391\n",
      "Epoch 13/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 2.6356 - accuracy: 0.4839\n",
      "Epoch 14/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 2.3545 - accuracy: 0.5308\n",
      "Epoch 15/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 2.0987 - accuracy: 0.5747\n",
      "Epoch 16/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 1.8709 - accuracy: 0.6166\n",
      "Epoch 17/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 1.6694 - accuracy: 0.6538\n",
      "Epoch 18/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 1.4927 - accuracy: 0.6886\n",
      "Epoch 19/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 1.3394 - accuracy: 0.7170\n",
      "Epoch 20/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 1.2069 - accuracy: 0.7440\n",
      "Epoch 21/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 1.0924 - accuracy: 0.7669\n",
      "Epoch 22/50\n",
      "1163/1163 [==============================] - 17s 15ms/step - loss: 0.9940 - accuracy: 0.7865\n",
      "Epoch 23/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 0.9092 - accuracy: 0.8029\n",
      "Epoch 24/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.8364 - accuracy: 0.8182\n",
      "Epoch 25/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 0.7737 - accuracy: 0.8303\n",
      "Epoch 26/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 0.7189 - accuracy: 0.8418\n",
      "Epoch 27/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.6720 - accuracy: 0.8512\n",
      "Epoch 28/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.6311 - accuracy: 0.8602\n",
      "Epoch 29/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.5955 - accuracy: 0.8677\n",
      "Epoch 30/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.5642 - accuracy: 0.8729\n",
      "Epoch 31/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.5364 - accuracy: 0.8794\n",
      "Epoch 32/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.5118 - accuracy: 0.8830\n",
      "Epoch 33/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.4907 - accuracy: 0.8868\n",
      "Epoch 34/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.4707 - accuracy: 0.8913\n",
      "Epoch 35/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.4536 - accuracy: 0.8940\n",
      "Epoch 36/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.4375 - accuracy: 0.8971\n",
      "Epoch 37/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.4239 - accuracy: 0.9003\n",
      "Epoch 38/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.4106 - accuracy: 0.9024\n",
      "Epoch 39/50\n",
      "1163/1163 [==============================] - 17s 14ms/step - loss: 0.3988 - accuracy: 0.9055\n",
      "Epoch 40/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3881 - accuracy: 0.9062\n",
      "Epoch 41/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3778 - accuracy: 0.9084\n",
      "Epoch 42/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3686 - accuracy: 0.9092\n",
      "Epoch 43/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3605 - accuracy: 0.9115\n",
      "Epoch 44/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3522 - accuracy: 0.9133\n",
      "Epoch 45/50\n",
      "1163/1163 [==============================] - 16s 13ms/step - loss: 0.3448 - accuracy: 0.9145\n",
      "Epoch 46/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3384 - accuracy: 0.9164\n",
      "Epoch 47/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3314 - accuracy: 0.9173\n",
      "Epoch 48/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3258 - accuracy: 0.9182\n",
      "Epoch 49/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3202 - accuracy: 0.9196\n",
      "Epoch 50/50\n",
      "1163/1163 [==============================] - 16s 14ms/step - loss: 0.3148 - accuracy: 0.9206\n",
      "\n",
      "CPU times: user 53min 18s, sys: 14min 21s, total: 1h 7min 40s\n",
      "Wall time: 13min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train CBOW model\n",
    "for cbow in cbow_models:\n",
    "    cbow.fit(X_cbow, y_cbow, batch_size=64, epochs=50, verbose=1)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tIzwSyBRNC-V"
   },
   "outputs": [],
   "source": [
    "\n",
    "for cbow in cbow_models:\n",
    "\n",
    "    weights = cbow.get_weights()\n",
    "\n",
    "    # Get the embedding matrix\n",
    "    embedding = weights[0]\n",
    "\n",
    "    # Get word embeddings for each word in the vocabulary, write to file\n",
    "    f = open(f'vectors_cbow_{len(embedding[0])}.txt', 'w')\n",
    "\n",
    "    # Create columns for the words and the values in the matrix, makes it easier to read as dataframe\n",
    "    columns = [\"word\"] + [f\"value_{i+1}\" for i in range(embedding.shape[1])]\n",
    "\n",
    "    # Start writing to the file, start with the column names\n",
    "    f.write(\" \".join(columns))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(word)\n",
    "        f.write(\" \")\n",
    "        f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOduYv-ENJU8",
    "outputId": "0e81d5c1-c5c1-46aa-e472-9085d93babe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 nearest words to 'king' are: \n",
      "('wishes', 0.3323208988812061)\n",
      "('pops', 0.31664041545272287)\n",
      "('prince', 0.3159132598837587)\n",
      "('flagship', 0.3070605956052026)\n",
      "('breathtaking', 0.2986077509119104)\n",
      "('astounded', 0.29825930003995144)\n",
      "('understands', 0.29567787820537805)\n",
      "('rub', 0.2925689882841785)\n",
      "('heineken', 0.29195099642702327)\n",
      "('oversee', 0.2893588965674602)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Loading pre-trained word embeddings into a dictionary or a matrix\n",
    "# word_vectors should be a dictionary where keys are words and values are their corresponding vectors\n",
    "# Or word_vectors can be a matrix where rows correspond to words and columns are vector dimensions\n",
    "\n",
    "\n",
    "\n",
    "word_vectors = {}\n",
    "i=0\n",
    "\n",
    "target_word = \"king\"\n",
    "\n",
    "\n",
    "with open(\"vectors_cbow_300.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = np.array([float(x) for x in parts[1:]])\n",
    "        word_vectors[word] = vector\n",
    "\n",
    "# Target word for which you want to find the k-nearest words\n",
    "\n",
    "\n",
    "# Calculate cosine similarities with all words in the vocabulary\n",
    "similarities = {}\n",
    "target_vector = word_vectors[target_word]\n",
    "for word, vector in word_vectors.items():\n",
    "    if word != target_word:\n",
    "        cosine_sim = cosine_similarity([target_vector], [vector])\n",
    "        similarities[word] = cosine_sim[0][0]\n",
    "\n",
    "# Sort the words by their cosine similarity scores in descending order\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top-k words as the k-nearest words\n",
    "k = 10  # Number of nearest words you want to find\n",
    "nearest_words = [(word,e) for word, e in sorted_similarities[:k]]\n",
    "\n",
    "# Print the k-nearest words\n",
    "print(f\"The {k} nearest words to '{target_word}' are: \")\n",
    "for i in (nearest_words):\n",
    "    print(i)\n",
    "\n",
    "\n",
    "cbow_word_emd = word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0TRjJt_BzEl"
   },
   "source": [
    "To get the word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hD90GjaOB14b",
    "outputId": "af0dfd63-ecb2-4b3b-f558-d4f9ab4e8b1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9321, 9321)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipgram_word_emd),len(cbow_word_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAu2hbWgDh8H",
    "outputId": "5c04dc9d-6349-4c20-9ceb-34252ee51371"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09209947]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([skipgram_word_emd['king']], [cbow_word_emd['king']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbCqkmPZFxwk",
    "outputId": "18c0ed2a-4069-4ccd-eceb-b08fb80c2c3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03437119]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([skipgram_word_emd['queen']], [cbow_word_emd['queen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WqqC9hlDtgs",
    "outputId": "69c5c9fc-edcb-495f-e887-32ed151d0477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24368201]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([skipgram_word_emd['king']], [skipgram_word_emd['queen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dn76dASUD4FT",
    "outputId": "44d86057-a81e-4a40-d5b0-f420f4b01810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26642819]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([cbow_word_emd['king']], [cbow_word_emd['queen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
